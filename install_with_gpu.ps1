$env:CMAKE_ARGS="-DLLAMA_CUBLAS=on"
pip install llama-cpp-python  -v --upgrade --force-reinstall --no-cache-dir 